# 일일 크롤링 자동화 실행 계획

## 현재 상태 분석

| 스크립트 | 역할 | 저장 경로 | Selenium 사용 |
|---|---|---|---|
| `crawling_english_saying.py` | 영어 명언 수집 (Hackers영어) | `~/Desktop/{YYYY}년 영어 명언 모음.txt` | ❌ (requests) |
| `run_combined.py` | 뉴스 크롤링 통합 실행 | `C:\news\` 하위 폴더들 | - |
| `run_headline_crawling.py` | 네이버 헤드라인 | `C:\news\headlines\{YYYY}\{MM}\` | ✅ → 제거 |
| `run_economics_crawling.py` | 경제 뉴스 | `C:\news\economics\{YYYY}\{MM}\` | ✅ → 제거 |
| `run_opinions_crawling.py` | 사설 모음 | `C:\news\opinions\{YYYY}\{MM}\` | ✅ → 제거 |
| `run_eng_stock_check.py` | 영문 주식 뉴스 (finviz) | `C:\news\stock_news\{YYYY}\{MM}\` | ✅ → 제거 |

---

## 핵심 변경: Selenium → requests + BeautifulSoup 전환

크롬 드라이버 없이 실행 가능하도록 모든 스크립트를 `requests` + `BeautifulSoup` 기반으로 전환한다.

### 전환 전략 (스크립트별)

#### 1) run_headline_crawling.py

**현재 (Selenium)**
- `driver.get('https://news.naver.com/')` → 탭 클릭 → 헤드라인 더보기 클릭 → 요소 추출
- 개별 기사 URL 방문하여 작성일/수정일 추출

**전환 후 (requests + BS4)**
- 각 섹션별 직접 URL 사용 (탭 클릭 대체)
  - 경제: `https://news.naver.com/section/101`
  - IT/과학: `https://news.naver.com/section/105`
  - 세계: `https://news.naver.com/section/104`
  - 정치: `https://news.naver.com/section/100`
  - 사회: `https://news.naver.com/section/102`
  - 생활/문화: `https://news.naver.com/section/103`
- `requests.get(url)` → `BeautifulSoup` 파싱
- 헤드라인 요소: `section_component as_section_headline` 클래스 파싱
- 개별 기사 URL도 `requests.get()` + BS4로 작성일/수정일 추출

**주의사항**
- 네이버 뉴스 서버 렌더링(SSR)으로 JavaScript 없이도 HTML에 데이터 존재
- User-Agent 헤더 필수 (봇 차단 방지)
- 요청 간 `time.sleep(1)` 로 서버 부하 방지

#### 2) run_economics_crawling.py

**현재 (Selenium)**
- 경제 탭 클릭 → 서브섹션 nav 추출 → 각 서브섹션 방문 → 기사 추출 → 기사 상세 방문

**전환 후 (requests + BS4)**
- `https://news.naver.com/section/101` 직접 접근
- `ct_snb_nav` 클래스 내 서브섹션 링크 파싱
- 각 서브섹션 URL을 `requests.get()` → `section_latest` 내 기사 추출
- 중복 검사 로직 (SequenceMatcher) 그대로 유지
- 기사 상세 페이지도 `requests.get()` + BS4

#### 3) run_opinions_crawling.py ⚠️ 가장 큰 변경

**현재 (Selenium)**
- 사설 페이지 → **무한 스크롤**로 전체 로딩 → 필터링 → 개별 기사 방문

**전환 후 (requests + BS4)**
- 무한 스크롤 대체 방법:
  - **방법 A**: 네이버 뉴스 사설 API 엔드포인트 활용 (존재할 경우)
    - `https://news.naver.com/opinion/editorial` 의 XHR 요청 분석
    - 페이지네이션 파라미터로 반복 요청
  - **방법 B**: 직접 URL 페이지네이션
    - `https://news.naver.com/opinion/editorial?page=1`, `?page=2` ...
    - 더 이상 데이터가 없을 때까지 반복
  - **방법 C** (fallback): 첫 페이지 로드만으로 충분한 사설 수집
    - 첫 SSR 응답에 포함된 사설만 수집 (대부분의 당일 사설 포함)
- 대상 언론사 필터링 로직 그대로 유지
- 개별 사설 기사도 `requests.get()` + BS4

#### 4) run_eng_stock_check.py

**현재 (Selenium)**
- finviz.com 뉴스 페이지 → 뉴스 목록 추출 → 각 소스별 기사 방문

**전환 후 (requests + BS4)**
- `requests.get('https://finviz.com/news.ashx?v=3')` → BS4 파싱
- finviz는 SSR 페이지이므로 직접 파싱 가능
- 개별 기사 URL 방문 (Yahoo Finance, PR Newswire 등)도 `requests.get()` + BS4
- 소스별 파싱 로직 (Yahoo, PRNewswire, BusinessWire 등) 유지하되 셀렉터를 BS4 방식으로 변환

**주의사항**
- finviz User-Agent 검증 엄격 → 브라우저 UA 헤더 필수
- 일부 뉴스 소스(Yahoo Finance 등)가 JavaScript 렌더링 필요할 수 있음
  → 이 경우 기사 본문 없이 제목/URL만 수집 (graceful degradation)

---

## 공통 HTTP 유틸리티 모듈: `http_utils.py` (신규)

모든 스크립트에서 공유할 HTTP 요청 유틸리티를 별도 모듈로 분리:

```python
# http_utils.py
import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
}

def fetch_page(url, timeout=10):
    """URL에서 HTML을 가져와 BeautifulSoup 객체로 반환"""
    response = requests.get(url, headers=HEADERS, timeout=timeout)
    response.raise_for_status()
    return BeautifulSoup(response.text, "html.parser")

def fetch_text(url, timeout=10):
    """URL에서 HTML 텍스트만 반환"""
    response = requests.get(url, headers=HEADERS, timeout=timeout)
    response.raise_for_status()
    return response.text
```

---

## 구현할 것: `daily_runner.py` (메인 실행 스크립트)

### Step 1. 인터넷 연결 확인 로직

```
함수: check_internet_connection()
├── google.com 에 HTTP HEAD 요청 (timeout=3초)
├── 성공 → True 반환
└── 실패 → False 반환

함수: wait_for_internet(max_retries=5, interval=5)
├── 1차 시도: check_internet_connection()
│   ├── 성공 → 크롤링 진행
│   └── 실패 → 5초 대기 후 재시도
├── 2차~4차 시도: 동일 (5초 간격)
└── 5차 시도 실패 시:
    ├── 콘솔 출력: "인터넷 연결을 확인할 수 없습니다. 크롤링을 중단합니다."
    ├── 로그 파일에 기록
    └── 프로그램 종료 (exit code 1)
```

- 프로그램 시작 시 1회 실행
- 각 크롤링 단계 사이에도 연결 확인 (단, 이미 통과 시 재시도 로직 없이 1회만)

### Step 2. 영어 명언 수집 (우선 실행)

```
실행 순서:
1. crawling_english_saying.py 의 insert_latest_quote() 호출
2. 결과: ~/Desktop/{YYYY}년 영어 명언 모음.txt 에 직접 기록
   → 바탕화면에 실제 파일로 존재 (현재 로직 유지)
3. 성공/실패 로그 출력
```

- 현재 코드가 이미 requests + BS4 사용 → 변경 불필요
- 파일이 없는 경우(연초 등) 새 파일 생성 로직 추가 필요

### Step 3. 뉴스 크롤링 실행

```
실행 순서:
1. 인터넷 연결 재확인 (1회)
2. run_combined.py 의 main() 호출
   → run_headline_crawling.main()      (requests + BS4)
   → run_economics_crawling.main()     (requests + BS4)
   → run_opinions_crawling.main()      (requests + BS4)
   → run_eng_stock_check.main()        (requests + BS4)
3. 결과: C:\news\ 하위에 오늘 날짜 파일들 생성
```

### Step 4. 바탕화면 바로가기 생성

```
함수: create_news_shortcut()
├── 바탕화면 경로: ~/Desktop
├── 대상 폴더: C:\news
├── 바로가기 파일: ~/Desktop/뉴스 모음.lnk
├── 방법: winshell 또는 win32com.client 사용
│   import win32com.client
│   shell = win32com.client.Dispatch("WScript.Shell")
│   shortcut = shell.CreateShortCut(shortcut_path)
│   shortcut.TargetPath = "C:\\news"
│   shortcut.save()
└── 이미 존재하면 생성 건너뛰기
```

- 뉴스 폴더(`C:\news`)는 바로가기(.lnk)로 바탕화면에 배치
- 영어 명언 파일은 이미 실제 파일로 바탕화면에 존재

### Step 5. 로깅 및 결과 보고

```
실행 완료 후:
├── 콘솔에 요약 출력
│   ├── 인터넷 연결: ✓
│   ├── 영어 명언 수집: ✓ (날짜)
│   ├── 헤드라인 크롤링: ✓
│   ├── 경제 뉴스 크롤링: ✓
│   ├── 사설 크롤링: ✓
│   ├── 영문 주식 뉴스: ✓
│   └── 바탕화면 바로가기: ✓
└── 로그 파일: C:\news\logs\{YYYY-MM-DD}_실행로그.txt
```

---

## 전체 실행 흐름도

```
daily_runner.py 시작
    │
    ▼
[인터넷 연결 확인] ──실패──→ 5초 대기 → 재시도 (최대 5회)
    │                                         │
    │ 성공                              5회 실패 → 종료 알림
    ▼
[영어 명언 수집] ← crawling_english_saying.py (requests+BS4, 변경 없음)
    │                 (바탕화면에 실제 파일)
    ▼
[인터넷 연결 재확인] ──실패──→ 경고 후 건너뛰기
    │
    │ 성공
    ▼
[뉴스 크롤링] ← run_combined.py (전부 requests+BS4로 전환)
    │              (C:\news\ 에 저장)
    ▼
[바탕화면 바로가기 생성] → ~/Desktop/뉴스 모음.lnk → C:\news
    │
    ▼
[실행 결과 요약 출력]
    │
    ▼
종료
```

---

## 수정이 필요한 기존 파일

| 파일 | 수정 내용 |
|---|---|
| `crawling_english_saying.py` | 연초에 파일이 없을 때 새 파일 자동 생성 로직 추가 |
| `run_headline_crawling.py` | **Selenium 전체 제거** → requests+BS4, 섹션별 직접 URL 사용 |
| `run_economics_crawling.py` | **Selenium 전체 제거** → requests+BS4, 서브섹션 직접 접근 |
| `run_opinions_crawling.py` | **Selenium 전체 제거** → requests+BS4, 무한스크롤→API/페이지네이션 |
| `run_eng_stock_check.py` | **Selenium 전체 제거** → requests+BS4, 소스별 파서 유지 |
| `run_combined.py` | 각 크롤러 실행 시 try-except 래핑 (하나 실패해도 나머지 계속) |

## 신규 생성 파일

| 파일 | 역할 |
|---|---|
| `daily_runner.py` | 메인 실행 스크립트 (인터넷 확인 + 순차 실행 + 바로가기 + 로깅) |
| `http_utils.py` | 공통 HTTP 유틸리티 (requests+BS4 래퍼, 헤더, fetch_page 등) |

---

## 필요 패키지 (최종)

- `requests` — HTTP 요청 (인터넷 확인 + 모든 크롤링)
- `beautifulsoup4` — HTML 파싱 (모든 크롤링)
- `pywin32` — 바탕화면 바로가기 생성 (`win32com.client`)

### 제거되는 패키지

- ~~`selenium`~~ — 더 이상 불필요
- ~~`webdriver-manager`~~ — 더 이상 불필요
- ~~Chrome / ChromeDriver~~ — 더 이상 불필요

---

## Selenium 제거 시 리스크 & 대응

| 리스크 | 영향 스크립트 | 대응 방안 |
|---|---|---|
| JS 렌더링 필요 페이지 | opinions (무한스크롤) | API 엔드포인트 탐색 또는 첫 페이지만 수집 |
| 봇 차단 (403) | finviz, Yahoo Finance | 브라우저 UA + Referer 헤더, 요청 간 딜레이 |
| 기사 본문 JS 로딩 | Yahoo Finance 등 일부 | 본문 수집 불가 시 제목/URL만 수집 (graceful) |
| HTML 구조 변경 | 모든 스크립트 | 파서 실패 시 로그 남기고 다음 진행 (try-except) |
